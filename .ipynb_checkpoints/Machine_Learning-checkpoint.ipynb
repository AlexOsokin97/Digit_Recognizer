{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for machine learning algorithm's useage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, training_features, training_labels,metric, kfolds):\n",
    "    '''\n",
    "    A function that applies cross_validation on given machine learning algorithm, data and number of data splits\n",
    "    \n",
    "    input:\n",
    "            model (sklearn machine learning algorithm api): LogisticRegression, XGBoost, etc..\n",
    "            training_features (DataFrame): (X_train)\n",
    "            training_labels (DataFrame): (y_train)\n",
    "            metric (String, sklearn scoring metrics api): \n",
    "            kfolds (int): number of splits to perform on the datasets\n",
    "    \n",
    "    output:\n",
    "            scores (list): a list with scoring values for each K split\n",
    "            average_score (float): the mean of scores\n",
    "            \n",
    "    '''\n",
    "    scores = cross_val_score(estimator=model, X=training_features.values, y=training_labels.values.ravel(), scoring=metric, cv=kfolds, verbose=3, n_jobs=-1)\n",
    "    average_score = np.mean(scores)\n",
    "    return scores, average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(model, training_features, training_labels,metric, hyper_params, cv):\n",
    "    '''\n",
    "    A function that applies GridSearch (hyper parameter tuning) for a given machine learning algorithm.\n",
    "    \n",
    "    *In order to get the best of this function it is recommended that you'll use it on algorithms with many hyper parameters*\n",
    "    \n",
    "    input:\n",
    "            model (sklearn machine learning algorithm api): LogisticRegression, XGBoost, etc..\n",
    "            training_features (DataFrame): (X_train)\n",
    "            training_labels (DataFrame): (y_train)\n",
    "            metric (String, sklearn scoring metrics api): \n",
    "            hyper_params (list): A list which contains a dictionary with it's keys as names of a model's hyper parameters and \n",
    "                                 values to test on\n",
    "            cv (int): cross validation splitting strategy (3-fold, 5-fold). *for faster performance choose 3 fold cv*\n",
    "    \n",
    "    output: \n",
    "            best_find (machine learning model): returns the given machine learning algorithm with the best hyper parameters\n",
    "            best_score (float): returns the best score achieved by the model with the best hyper parameters\n",
    "            \n",
    "    '''\n",
    "    \n",
    "    grid_search = GridSearchCV(model, param_grid=hyper_params, scoring=metric, verbose=3, cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(training_features.values, training_labels.values.ravel())\n",
    "    best_find = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    return best_find, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(trained_model, testing_features, testing_labels, metrics = []):\n",
    "    '''\n",
    "    A functions that applies multiple scoring metrics given by the user on the testing set\n",
    "    \n",
    "    input:\n",
    "            trained_model (sklearn machine learning algorithm api): An already trained machine learning algorithm\n",
    "            testing_features (DataFrame): (X_test) \n",
    "            testing_label (DataFrame): (y_test)\n",
    "            metrics (list of sklearn metric api): a list which contains the desired scoring metrics\n",
    "    \n",
    "    output:\n",
    "            scores (dictionary): a dictionary which contains the scoring method as a key and the score as value\n",
    "            \n",
    "    '''\n",
    "    scores = {}\n",
    "    num_metrics = 1\n",
    "    predictions = trained_model.predict(testing_features.values)\n",
    "    for metric in metrics:\n",
    "        \n",
    "        if str(metric) == str(f1_score):\n",
    "            score = f1_score(testing_labels.values.ravel(), predictions, average='micro')\n",
    "            scores[num_metrics] = score\n",
    "            num_metrics += 1\n",
    "        else:\n",
    "            score = metric(testing_labels.values.ravel(), predictions)\n",
    "            scores[num_metrics] = score\n",
    "            num_metrics += 1\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./Engineered_Datasets/Engineered_X_train.csv')\n",
    "X_test = pd.read_csv('./Engineered_Datasets/Engineered_X_test.csv')\n",
    "y_train = pd.read_csv('./Engineered_Datasets/X_train_label.csv')\n",
    "y_test = pd.read_csv('./Engineered_Datasets/X_test_label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=42, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.901, total=   5.2s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.892, total=   4.8s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    9.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.904, total=   4.9s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.896, total=   5.4s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.905, total=   5.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.896, total=   5.2s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.901, total=   4.6s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.901, total=   4.7s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.899, total=   4.8s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.903, total=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   49.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.90059524, 0.89196429, 0.90446429, 0.89642857, 0.90505952,\n",
       "        0.89613095, 0.90059524, 0.90119048, 0.89940476, 0.90327381]),\n",
       " 0.8999107142857143)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(lr, X_train, y_train, 'accuracy', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 800 candidates, totalling 2400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 28.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed: 43.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1536 tasks      | elapsed: 68.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2016 tasks      | elapsed: 106.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2400 out of 2400 | elapsed: 145.1min finished\n"
     ]
    }
   ],
   "source": [
    "lr_param_grid = [    \n",
    "    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C' : np.logspace(-4, 4, 10),\n",
    "    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
    "    'max_iter' : [500, 750, 1000, 1250]\n",
    "    }\n",
    "]\n",
    "\n",
    "best_lr, best_score = hyper_tuning(lr, X_train, y_train, 'accuracy', lr_param_grid, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression model is: \n",
      " LogisticRegression(C=1291.5496650148827, class_weight=None, dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=500, multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=42, solver='newton-cg', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Best Score: 0.9060714285714285%\n"
     ]
    }
   ],
   "source": [
    "print('Best logistic regression model is: \\n', best_lr)\n",
    "print('Best Score: ' + str(best_score) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([[787,   0,   3,   0,   3,  15,   5,   0,   3,   0],\n",
       "        [  0, 896,   4,   0,   1,   3,   0,   2,   3,   0],\n",
       "        [  5,   7, 742,  20,  17,   7,  15,   8,  23,   2],\n",
       "        [  2,   4,  23, 791,   1,  52,   4,  12,  28,  20],\n",
       "        [  1,   1,   7,   2, 766,   2,  14,   4,   6,  36],\n",
       "        [  4,   1,   7,  31,  16, 588,  20,   4,  23,   8],\n",
       "        [ 10,   2,   6,   1,  12,   6, 736,   2,   9,   1],\n",
       "        [  0,   1,  15,   5,  14,   6,   0, 812,   4,  36],\n",
       "        [  4,   7,   9,  25,   4,  24,   8,   4, 743,   7],\n",
       "        [  1,   2,   4,  16,  27,  11,   0,  34,   4, 739]], dtype=int64),\n",
       " 2: 0.9047619047619048,\n",
       " 3: 0.9047619047619048}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance(best_lr, X_test, y_test, [confusion_matrix, accuracy_score, f1_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.944, total=  16.3s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   16.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.939, total=  16.3s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   32.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.950, total=  16.5s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.942, total=  16.5s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.945, total=  16.5s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.946, total=  16.4s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.948, total=  16.5s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.947, total=  16.8s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.948, total=  16.6s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.953, total=  16.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.94434524, 0.93869048, 0.95029762, 0.94166667, 0.94494048,\n",
       "        0.94613095, 0.94821429, 0.94672619, 0.94791667, 0.95297619]),\n",
       " 0.9461904761904762)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(rfc, X_train, y_train, 'accuracy', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_params_grid = [\n",
    "    {'n_estimators': [100,200,300],\n",
    "     'min_samples_split': [2, 12, 41, 94],\n",
    "     'min_samples_leaf' : [1, 15, 67, 82],\n",
    "     'criterion': ['gini', 'entropy']\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=-1)]: Done 288 out of 288 | elapsed: 16.4min finished\n"
     ]
    }
   ],
   "source": [
    "best_rfc, best_score = hyper_tuning(rfc, X_train, y_train, 'accuracy', rfc_params_grid, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression model is: \n",
      " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=300,\n",
      "                       n_jobs=None, oob_score=False, random_state=23, verbose=0,\n",
      "                       warm_start=False)\n",
      "Best Score: 0.9440476190476191%\n"
     ]
    }
   ],
   "source": [
    "print('Best logistic regression model is: \\n', best_rfc)\n",
    "print('Best Score: ' + str(best_score) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([[799,   0,   1,   2,   1,   2,   9,   0,   1,   1],\n",
       "        [  0, 896,   5,   2,   1,   2,   1,   1,   1,   0],\n",
       "        [  4,   4, 797,  11,   8,   2,   2,   6,  11,   1],\n",
       "        [  5,   1,  10, 857,   1,  18,   2,  11,  19,  13],\n",
       "        [  2,   0,   3,   0, 789,   0,  10,   4,   4,  27],\n",
       "        [  2,   0,   4,   9,   5, 659,  12,   1,   8,   2],\n",
       "        [  8,   0,   1,   0,   0,   6, 768,   0,   2,   0],\n",
       "        [  1,   6,  14,   0,   8,   0,   0, 841,   3,  20],\n",
       "        [  1,   2,   4,  21,   5,  18,   5,   3, 770,   6],\n",
       "        [  1,   3,   3,  20,  13,   5,   0,  16,   5, 772]], dtype=int64),\n",
       " 2: 0.9461904761904761,\n",
       " 3: 0.9461904761904761}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance(best_rfc, X_test, y_test, [confusion_matrix, accuracy_score, f1_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
