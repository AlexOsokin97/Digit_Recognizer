{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for machine learning algorithm's useage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, training_features, training_labels,metric, kfolds):\n",
    "    '''\n",
    "    A function that applies cross_validation on given machine learning algorithm, data and number of data splits\n",
    "    \n",
    "    input:\n",
    "            model (sklearn machine learning algorithm api): LogisticRegression, XGBoost, etc..\n",
    "            training_features (DataFrame): (X_train)\n",
    "            training_labels (DataFrame): (y_train)\n",
    "            metric (String, sklearn scoring metrics api): \n",
    "            kfolds (int): number of splits to perform on the datasets\n",
    "    \n",
    "    output:\n",
    "            scores (list): a list with scoring values for each K split\n",
    "            average_score (float): the mean of scores\n",
    "            \n",
    "    '''\n",
    "    scores = cross_val_score(estimator=model, X=training_features.values, y=training_labels.values.ravel(), scoring=metric, cv=kfolds)\n",
    "    average_score = np.mean(scores)\n",
    "    return scores, average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(model, training_features, training_labels,metric, hyper_params, cv):\n",
    "    '''\n",
    "    A function that applies GridSearch (hyper parameter tuning) for a given machine learning algorithm.\n",
    "    \n",
    "    *In order to get the best of this function it is recommended that you'll use it on algorithms with many hyper parameters*\n",
    "    \n",
    "    input:\n",
    "            model (sklearn machine learning algorithm api): LogisticRegression, XGBoost, etc..\n",
    "            training_features (DataFrame): (X_train)\n",
    "            training_labels (DataFrame): (y_train)\n",
    "            metric (String, sklearn scoring metrics api): \n",
    "            hyper_params (list): A list which contains a dictionary with it's keys as names of a model's hyper parameters and \n",
    "                                 values to test on\n",
    "            cv (int): cross validation splitting strategy (3-fold, 5-fold). *for faster performance choose 3 fold cv*\n",
    "    \n",
    "    output: \n",
    "            best_find (machine learning model): returns the given machine learning algorithm with the best hyper parameters\n",
    "            best_score (float): returns the best score achieved by the model with the best hyper parameters\n",
    "            \n",
    "    '''\n",
    "    \n",
    "    grid_search = GridSearchCV(model, param_grid=hyper_params, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(training_features.values, training_labels.values.ravel())\n",
    "    best_find = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    return best_find, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(trained_model, testing_features, testing_labels, metrics = []):\n",
    "    '''\n",
    "    A functions that applies multiple scoring metrics given by the user on the testing set\n",
    "    \n",
    "    input:\n",
    "            trained_model (sklearn machine learning algorithm api): An already trained machine learning algorithm\n",
    "            testing_features (DataFrame): (X_test) \n",
    "            testing_label (DataFrame): (y_test)\n",
    "            metrics (list of sklearn metric api): a list which contains the desired scoring metrics\n",
    "    \n",
    "    output:\n",
    "            scores (dictionary): a dictionary which contains the scoring method as a key and the score as value\n",
    "            \n",
    "    '''\n",
    "    scores = {}\n",
    "    num_metrics = 1\n",
    "    predictions = trained_model.predict(testing_features.values)\n",
    "    for metric in metrics:\n",
    "        \n",
    "        if str(metric) == str(f1_score):\n",
    "            score = f1_score(testing_labels.values.ravel(), predictions, average='micro')\n",
    "            scores[num_metrics] = score\n",
    "            num_metrics += 1\n",
    "        else:\n",
    "            score = metric(testing_labels.values.ravel(), predictions)\n",
    "            scores[num_metrics] = score\n",
    "            num_metrics += 1\n",
    "        \n",
    "    return scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
